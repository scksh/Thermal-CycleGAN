{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad6ed19-c6f0-403f-8bbe-f92d89e0e1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1단계] 빈 라벨 파일 개수: 0\n",
      "[1단계] 유효 파일 수 (세그 or bbox 포함): 56\n",
      "[2단계] Segmentation 좌표로 생성된 마스크 수: 3\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 11.89 GiB of which 242.12 MiB is free. Process 2362 has 233.11 MiB memory in use. Process 36891 has 6.30 GiB memory in use. Including non-PyTorch memory, this process has 4.86 GiB memory in use. Of the allocated memory 4.60 GiB is allocated by PyTorch, and 80.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 125\u001b[0m\n\u001b[1;32m    123\u001b[0m height, width \u001b[38;5;241m=\u001b[39m image_rgb\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    124\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((height, width), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m--> 125\u001b[0m predictor\u001b[38;5;241m.\u001b[39mset_image(image_rgb)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(label_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    128\u001b[0m     lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/segment_anything/predictor.py:60\u001b[0m, in \u001b[0;36mSamPredictor.set_image\u001b[0;34m(self, image, image_format)\u001b[0m\n\u001b[1;32m     57\u001b[0m input_image_torch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(input_image, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     58\u001b[0m input_image_torch \u001b[38;5;241m=\u001b[39m input_image_torch\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()[\u001b[38;5;28;01mNone\u001b[39;00m, :, :, :]\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_torch_image(input_image_torch, image\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/segment_anything/predictor.py:89\u001b[0m, in \u001b[0;36mSamPredictor.set_torch_image\u001b[0;34m(self, transformed_image, original_image_size)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(transformed_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n\u001b[1;32m     88\u001b[0m input_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpreprocess(transformed_image)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mimage_encoder(input_image)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_image_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/segment_anything/modeling/image_encoder.py:112\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 112\u001b[0m     x \u001b[38;5;241m=\u001b[39m blk(x)\n\u001b[1;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/segment_anything/modeling/image_encoder.py:174\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m     H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    172\u001b[0m     x, pad_hw \u001b[38;5;241m=\u001b[39m window_partition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m--> 174\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(x)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/segment_anything/modeling/image_encoder.py:234\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    231\u001b[0m attn \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_pos:\n\u001b[0;32m--> 234\u001b[0m     attn \u001b[38;5;241m=\u001b[39m add_decomposed_rel_pos(attn, q, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_pos_h, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_pos_w, (H, W), (H, W))\n\u001b[1;32m    236\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    237\u001b[0m x \u001b[38;5;241m=\u001b[39m (attn \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/segment_anything/modeling/image_encoder.py:358\u001b[0m, in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    354\u001b[0m rel_h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhwc,hkc->bhwk\u001b[39m\u001b[38;5;124m\"\u001b[39m, r_q, Rh)\n\u001b[1;32m    355\u001b[0m rel_w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhwc,wkc->bhwk\u001b[39m\u001b[38;5;124m\"\u001b[39m, r_q, Rw)\n\u001b[1;32m    357\u001b[0m attn \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 358\u001b[0m     attn\u001b[38;5;241m.\u001b[39mview(B, q_h, q_w, k_h, k_w) \u001b[38;5;241m+\u001b[39m rel_h[:, :, :, :, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m+\u001b[39m rel_w[:, :, :, \u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[1;32m    359\u001b[0m )\u001b[38;5;241m.\u001b[39mview(B, q_h \u001b[38;5;241m*\u001b[39m q_w, k_h \u001b[38;5;241m*\u001b[39m k_w)\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 11.89 GiB of which 242.12 MiB is free. Process 2362 has 233.11 MiB memory in use. Process 36891 has 6.30 GiB memory in use. Including non-PyTorch memory, this process has 4.86 GiB memory in use. Of the allocated memory 4.60 GiB is allocated by PyTorch, and 80.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "# ===== 설정 =====\n",
    "image_folder = r\"/home/ricky/Data/datasets/person/images\"\n",
    "label_folder = r\"/home/ricky/Data/datasets/person/labels\"\n",
    "images_a_folder = r\"/home/ricky/Data/datasets/person/images_a\"\n",
    "images_seg_a_folder = r\"/home/ricky/Data/datasets/person/images_seg_a\"\n",
    "label_a_folder = r\"/home/ricky/Data/datasets/person/labels_a\"\n",
    "sam_checkpoint = r\"/home/ricky/Data/0527sam_yolov8/sam_vit_h_4b8939.pth\"\n",
    "target_class_id = 0\n",
    "\n",
    "os.makedirs(images_a_folder, exist_ok=True)\n",
    "os.makedirs(images_seg_a_folder, exist_ok=True)\n",
    "os.makedirs(label_a_folder, exist_ok=True)\n",
    "\n",
    "# ===== SAM 모델 로드 =====\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=sam_checkpoint).to(device)\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "# ===== 보조 함수 =====\n",
    "def imread_unicode(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return np.array(img.convert('RGB'))\n",
    "\n",
    "def parse_label_line(line):\n",
    "    try:\n",
    "        return list(map(float, line.strip().split()))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def is_segmentation_format(values):\n",
    "    return len(values) > 5\n",
    "\n",
    "def create_mask_from_segmentation(mask, values, img_size):\n",
    "    H, W = img_size\n",
    "    points = np.array([\n",
    "        (values[i] * W, values[i + 1] * H)\n",
    "        for i in range(1, len(values), 2)\n",
    "    ], dtype=np.int32)\n",
    "    if len(points) >= 3:\n",
    "        cv2.fillPoly(mask, [points], 255)\n",
    "\n",
    "# ====== Step 1: 빈 .txt 파일 카운트 ======\n",
    "empty_label_count = 0\n",
    "valid_filenames = []\n",
    "\n",
    "for filename in os.listdir(image_folder):\n",
    "    if not filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "        continue\n",
    "\n",
    "    label_name = os.path.splitext(filename)[0] + \".txt\"\n",
    "    label_path = os.path.join(label_folder, label_name)\n",
    "\n",
    "    if not os.path.exists(label_path):\n",
    "        continue\n",
    "\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    if not lines:\n",
    "        empty_label_count += 1\n",
    "        continue\n",
    "\n",
    "    valid_filenames.append(filename)\n",
    "\n",
    "print(f\"[1단계] 빈 라벨 파일 개수: {empty_label_count}\")\n",
    "print(f\"[1단계] 유효 파일 수 (세그 or bbox 포함): {len(valid_filenames)}\")\n",
    "\n",
    "# ====== Step 2: Seg 좌표로 마스크 생성 ======\n",
    "seg_mask_count = 0\n",
    "seg_filenames = []\n",
    "\n",
    "for filename in valid_filenames:\n",
    "    label_path = os.path.join(label_folder, os.path.splitext(filename)[0] + \".txt\")\n",
    "    image_path = os.path.join(image_folder, filename)\n",
    "\n",
    "    image = imread_unicode(image_path)\n",
    "    H, W = image.shape[:2]\n",
    "    mask = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    has_seg = False\n",
    "    for line in lines:\n",
    "        values = parse_label_line(line)\n",
    "        if values and int(values[0]) == target_class_id and is_segmentation_format(values):\n",
    "            create_mask_from_segmentation(mask, values, (H, W))\n",
    "            has_seg = True\n",
    "\n",
    "    if has_seg:\n",
    "        out_img = os.path.join(images_a_folder, filename)\n",
    "        out_mask = os.path.join(images_seg_a_folder, os.path.splitext(filename)[0] + \".png\")\n",
    "        cv2.imwrite(out_mask, mask)\n",
    "        shutil.copy2(image_path, out_img)\n",
    "        seg_mask_count += 1\n",
    "        seg_filenames.append(filename)\n",
    "\n",
    "print(f\"[2단계] Segmentation 좌표로 생성된 마스크 수: {seg_mask_count}\")\n",
    "\n",
    "# ====== Step 3: 나머지 bbox → SAM 변환 ======\n",
    "bbox_mask_count = 0\n",
    "\n",
    "for filename in valid_filenames:\n",
    "    if filename in seg_filenames:\n",
    "        continue  # 이미 처리된 seg는 제외\n",
    "\n",
    "    label_path = os.path.join(label_folder, os.path.splitext(filename)[0] + \".txt\")\n",
    "    image_path = os.path.join(image_folder, filename)\n",
    "\n",
    "    image_bgr = cv2.imread(image_path)\n",
    "    if image_bgr is None:\n",
    "        continue\n",
    "\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    height, width = image_rgb.shape[:2]\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    predictor.set_image(image_rgb)\n",
    "\n",
    "    with open(label_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    found = False\n",
    "    for i, line in enumerate(lines):\n",
    "        values = parse_label_line(line)\n",
    "        if values is None or int(values[0]) != target_class_id or is_segmentation_format(values):\n",
    "            continue\n",
    "\n",
    "        # bbox → SAM\n",
    "        _, x_center, y_center, w, h = values\n",
    "        x_c, y_c = x_center * width, y_center * height\n",
    "        box_w, box_h = w * width, h * height\n",
    "        x1 = int(x_c - box_w / 2)\n",
    "        y1 = int(y_c - box_h / 2)\n",
    "        x2 = int(x_c + box_w / 2)\n",
    "        y2 = int(y_c + box_h / 2)\n",
    "        input_box = np.array([x1, y1, x2, y2])\n",
    "\n",
    "        try:\n",
    "            masks, _, _ = predictor.predict(\n",
    "                box=input_box[None, :],\n",
    "                multimask_output=False\n",
    "            )\n",
    "            mask += masks[0].astype(np.uint8) * 255\n",
    "            found = True\n",
    "            print(f\"[3단계] bbox → SAM 변환 성공: {filename} (box {i})\")\n",
    "        except Exception as e:\n",
    "            print(f\"[에러] SAM 변환 실패: {filename} / {e}\")\n",
    "\n",
    "    if found:\n",
    "        out_img = os.path.join(images_a_folder, filename)\n",
    "        out_mask = os.path.join(images_seg_a_folder, os.path.splitext(filename)[0] + \".png\")\n",
    "        cv2.imwrite(out_mask, mask)\n",
    "        shutil.copy2(image_path, out_img)\n",
    "        bbox_mask_count += 1\n",
    "\n",
    "print(f\"[3단계] bbox → SAM 변환으로 생성된 마스크 수: {bbox_mask_count}\")\n",
    "\n",
    "\n",
    "# ====== Step 4: images_a에 대응하는 라벨을 labels_a에 복사 ======\n",
    "\n",
    "copied_count = 0\n",
    "for filename in os.listdir(images_a_folder):\n",
    "    if not filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "        continue\n",
    "\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "    label_name = base_name + \".txt\"\n",
    "\n",
    "    src_label = os.path.join(label_folder, label_name)\n",
    "    dst_label = os.path.join(label_a_folder, label_name)\n",
    "\n",
    "    if os.path.exists(src_label):\n",
    "        shutil.copy2(src_label, dst_label)\n",
    "        copied_count += 1\n",
    "    else:\n",
    "        print(f\"[경고] 라벨 파일 없음: {label_name}\")\n",
    "\n",
    "print(f\"[4단계] 라벨 복사 완료: 총 {copied_count}개\")\n",
    "\n",
    "\n",
    "# ====== 최종 요약 ======\n",
    "print(\"\\n===== 최종 요약 =====\")\n",
    "print(f\"전체 이미지 수: {len(os.listdir(image_folder))}\")\n",
    "print(f\"빈 txt 파일 수: {empty_label_count}\")\n",
    "print(f\"Seg 좌표 마스크 수: {seg_mask_count}\")\n",
    "print(f\"BBox → SAM 마스크 수: {bbox_mask_count}\")\n",
    "print(f\"총 마스크 생성 수: {seg_mask_count + bbox_mask_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3e41c-58b1-4525-9f5f-f4aade8c5819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
